{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e52986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage, BaseMessage\n",
    "from langchain.messages import RemoveMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from CONFIG import GROQ_MODEL, TEMPERATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c63fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "llm = ChatGroq(model=GROQ_MODEL, temperature=TEMPERATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f89828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class state_class(TypedDict):\n",
    "    messages: Annotated[list[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf1ef4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_node(state: state_class):\n",
    "    messages = state['messages']\n",
    "    response = llm.invoke(messages)\n",
    "    return {'messages': [response]}\n",
    "\n",
    "def remove_messages(state: state_class):\n",
    "    mess = state['messages']\n",
    "\n",
    "    if len(mess) > 10:\n",
    "        to_remove = mess[:6]\n",
    "        return {'messages': [RemoveMessage(id=m.id) for m in to_remove]}\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0454dd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = InMemorySaver()\n",
    "\n",
    "builder = StateGraph(state_class)\n",
    "builder.add_node('chat_node', chat_node)\n",
    "builder.add_node('remove_messages', remove_messages)\n",
    "\n",
    "builder.add_edge(START, 'chat_node')\n",
    "builder.add_edge('chat_node', 'remove_messages')\n",
    "builder.add_edge('remove_messages', END)\n",
    "\n",
    "graph = builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c323336",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'configurable': {'thread_id': 'thread_09'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6e41eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is your cut-date-knowledge', additional_kwargs={}, response_metadata={}, id='afd7f6b6-a8e5-47fc-b9e0-72bce2a0cab7'),\n",
       "  AIMessage(content='My knowledge cutoff is currently December 2023. This means that my training data is based on information available up to December 2023, and I may not be aware of events, developments, or updates that have occurred after that date.\\n\\nIn other words, my knowledge is \"frozen\" as of December 2023, and I won\\'t be able to provide information on very recent events, discoveries, or changes that have taken place after that date.\\n\\nHowever, my training data is based on a massive corpus of text, and I have been trained on a wide range of topics, including but not limited to:\\n\\n* General knowledge\\n* History\\n* Science\\n* Technology\\n* Arts\\n* Culture\\n* Entertainment\\n* And many more!\\n\\nSo, while I may not have the very latest information, I can still provide a vast amount of knowledge and insights on a wide range of topics. If you have any specific questions or topics you\\'d like to discuss, I\\'m here to help!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 336, 'total_tokens': 538, 'completion_time': 0.503583557, 'completion_tokens_details': None, 'prompt_time': 0.077324298, 'prompt_tokens_details': None, 'queue_time': 0.225342736, 'total_time': 0.580907855}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_93b5f9e564', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36c-709d-7d81-97a1-131ff568a915-0', usage_metadata={'input_tokens': 336, 'output_tokens': 202, 'total_tokens': 538}),\n",
       "  HumanMessage(content='which model do you used now?', additional_kwargs={}, response_metadata={}, id='b2b06575-bd19-4630-a4ee-59d910f5a54b'),\n",
       "  AIMessage(content=\"I'm an instance of a large language model, and my architecture is based on a transformer model. Specifically, I'm using a variant of the BERT (Bidirectional Encoder Representations from Transformers) model, which is a type of neural network designed for natural language processing tasks.\\n\\nMy model is a customized version of the BERT model, fine-tuned for conversational AI and dialogue generation. This allows me to understand and respond to a wide range of questions and topics, and to engage in conversation with users like you.\\n\\nSome key features of my model include:\\n\\n* **Transformer architecture**: This allows me to handle long-range dependencies and contextual relationships in language.\\n* **Self-attention mechanisms**: This enables me to focus on specific parts of the input text and weigh their importance when generating responses.\\n* **Large-scale pre-training**: My model was pre-trained on a massive corpus of text data, which allows me to learn patterns and relationships in language.\\n* **Fine-tuning for conversation**: My model was fine-tuned on a specific dataset of conversational text, which allows me to generate responses that are relevant and engaging.\\n\\nSome of the technical specs of my model include:\\n\\n* **Model size**: My model has around 1.5 billion parameters, which is relatively small compared to some other large language models.\\n* **Training data**: My model was trained on a dataset of around 45 terabytes of text data, which is equivalent to around 20 million books.\\n* **Compute resources**: My model is deployed on a cloud-based infrastructure, which allows me to scale up or down depending on demand.\\n\\nI hope that helps! Let me know if you have any other questions about my model or how I work.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 345, 'prompt_tokens': 554, 'total_tokens': 899, 'completion_time': 0.86806019, 'completion_tokens_details': None, 'prompt_time': 0.078886048, 'prompt_tokens_details': None, 'queue_time': 0.198943329, 'total_time': 0.946946238}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_43d97c5965', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36c-7419-71a3-8152-6cd184e969fc-0', usage_metadata={'input_tokens': 554, 'output_tokens': 345, 'total_tokens': 899}),\n",
       "  HumanMessage(content='oh ok', additional_kwargs={}, response_metadata={}, id='b05211f0-b057-42e3-a405-7bf5e30c62e7'),\n",
       "  AIMessage(content=\"It can be a bit technical, but I'm here to help and answer any questions you might have. Don't worry if you didn't understand everything, just let me know if there's something specific you'd like to know more about.\\n\\nSo, what would you like to talk about next? Would you like to:\\n\\n* Ask me a question on a specific topic?\\n* Play a game or have some fun?\\n* Discuss a particular interest or hobby?\\n* Or just chat and see where the conversation goes?\\n\\nLet me know, and I'll do my best to make it an enjoyable conversation!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 120, 'prompt_tokens': 910, 'total_tokens': 1030, 'completion_time': 0.328202637, 'completion_tokens_details': None, 'prompt_time': 0.235751876, 'prompt_tokens_details': None, 'queue_time': 0.216312396, 'total_time': 0.563954513}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36c-78ed-7ff3-a0d8-811da99b8a66-0', usage_metadata={'input_tokens': 910, 'output_tokens': 120, 'total_tokens': 1030}),\n",
       "  HumanMessage(content='so at the end, what you want to ask from me?', additional_kwargs={}, response_metadata={}, id='0674d0ed-7f38-487d-9b09-afb5353ca01d'),\n",
       "  AIMessage(content=\"At the end of our conversation, I'd like to ask:\\n\\n**How can I assist you further?**\\n\\nIn other words, is there something specific you'd like to know, discuss, or explore? Perhaps you have a question, a problem, or a topic you'd like to learn more about?\\n\\nI'm here to help, and I want to make sure that our conversation is helpful and meaningful to you. So, please feel free to ask me anything, and I'll do my best to provide a helpful and informative response.\\n\\nAdditionally, if you're willing, I'd love to hear your feedback on our conversation. You can help me improve by letting me know:\\n\\n* What did you enjoy about our conversation?\\n* Was there something that didn't quite work for you?\\n* Are there any topics or features you'd like to see me explore in the future?\\n\\nYour feedback is invaluable, and it will help me to refine my language understanding and generation capabilities.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 758, 'total_tokens': 952, 'completion_time': 0.577818347, 'completion_tokens_details': None, 'prompt_time': 0.040415205, 'prompt_tokens_details': None, 'queue_time': 0.222299088, 'total_time': 0.618233552}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_bebe2dd4fb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36c-7c55-7d43-817d-c382bcbb633c-0', usage_metadata={'input_tokens': 758, 'output_tokens': 194, 'total_tokens': 952})]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({'messages': HumanMessage(content=\"hi, how are you?\")}, config)\n",
    "graph.invoke({'messages': HumanMessage(content=\"do you know about me?\")}, config)\n",
    "graph.invoke({'messages': HumanMessage(content=\"ok my self adnan saeed\")}, config)\n",
    "graph.invoke({'messages': HumanMessage(content=\"what is your cut-date-knowledge\")}, config)\n",
    "graph.invoke({'messages': HumanMessage(content=\"which model do you used now?\")}, config)\n",
    "graph.invoke({'messages': HumanMessage(content=\"oh ok\")}, config)\n",
    "graph.invoke({'messages': HumanMessage(content=\"so at the end, what you want to ask from me?\")}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06c4b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config2 = {'configurable': {'thread_id': 'thread_010'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47cd1114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi, how are you?', additional_kwargs={}, response_metadata={}, id='48328c5c-6df1-4e59-b0d1-8a86b5a167a2'),\n",
       "  AIMessage(content=\"Hello. I'm just a computer program, so I don't have feelings or emotions like humans do. However, I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 41, 'total_tokens': 90, 'completion_time': 0.083535905, 'completion_tokens_details': None, 'prompt_time': 0.003694166, 'prompt_tokens_details': None, 'queue_time': 0.008721771, 'total_time': 0.087230071}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36e-2ee2-7533-8115-5b9d6a8b6f24-0', usage_metadata={'input_tokens': 41, 'output_tokens': 49, 'total_tokens': 90}),\n",
       "  HumanMessage(content='do you know about me?', additional_kwargs={}, response_metadata={}, id='dde89123-e47b-45e1-85e0-4990216a5c7b'),\n",
       "  AIMessage(content=\"I don't have any personal information about you. I'm a large language model, I don't have the ability to retain information about individual users or access any personal data. Each time you interact with me, it's a new conversation and I start from a blank slate.\\n\\nHowever, as we chat, I can learn more about your interests and preferences based on the topics you discuss and the questions you ask. But this information is not stored and is only used to provide more relevant and personalized responses during our conversation.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 104, 'prompt_tokens': 105, 'total_tokens': 209, 'completion_time': 0.314109937, 'completion_tokens_details': None, 'prompt_time': 0.009169335, 'prompt_tokens_details': None, 'queue_time': 0.008030667, 'total_time': 0.323279272}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_dae98b5ecb', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36e-2fe4-79e0-a594-2038e595e5fe-0', usage_metadata={'input_tokens': 105, 'output_tokens': 104, 'total_tokens': 209}),\n",
       "  HumanMessage(content='ok my self adnan saeed', additional_kwargs={}, response_metadata={}, id='e2684349-4328-4a70-a582-d6ef68a41f9b'),\n",
       "  AIMessage(content=\"Nice to meet you, Adnan Saeed! It's great that you've introduced yourself. I'll keep this in mind as we chat, and I'll do my best to provide you with helpful and interesting responses.\\n\\nSo, Adnan, what brings you here today? Is there something specific you'd like to talk about or ask about? I'm all ears (or rather, all text)!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 225, 'total_tokens': 307, 'completion_time': 0.420980449, 'completion_tokens_details': None, 'prompt_time': 0.011158407, 'prompt_tokens_details': None, 'queue_time': 0.008402643, 'total_time': 0.432138856}, 'model_name': 'llama-3.3-70b-versatile', 'system_fingerprint': 'fp_c06d5113ec', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019bb36e-3182-7cd3-8ee5-57de0f29c89c-0', usage_metadata={'input_tokens': 225, 'output_tokens': 82, 'total_tokens': 307})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.invoke({'messages': HumanMessage(content=\"hi, how are you?\")}, config2)\n",
    "graph.invoke({'messages': HumanMessage(content=\"do you know about me?\")}, config2)\n",
    "graph.invoke({'messages': HumanMessage(content=\"ok my self adnan saeed\")}, config2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c9ed9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config: It has 8 number of messages\n",
      "config2: It has 6 number of messages\n"
     ]
    }
   ],
   "source": [
    "print(f\"config: It has {len(graph.get_state(config).values['messages'])} number of messages\")\n",
    "print(f\"config2: It has {len(graph.get_state(config2).values['messages'])} number of messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff14753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
